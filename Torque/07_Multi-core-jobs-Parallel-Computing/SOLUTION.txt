Demo programs for chapter 7 of the HPC tutorial text

* T_hello.c: Posix threads demo
  - Compiling with the Intel compilers:
    + Load an intel module
    + icc -O2 -xHost -lpthread T_hello.c -o T_hello
      The -lpthread flag adds the pthread library to the list of libraries that
      need to be linked into the code.
  - Compiling with the GNU compilers
    + Load a GCC, foss or intel module (all three contain the gcc compiler)
    + gcc -O3 -march=native -lpthread T_hello.c -o T_hello
  - Run with the included job script T_hello.slurm (Intel compiler or GCC
    from the Intel module, replace modules for other compilers) on Slurm systems.
  - T_hello.pbs is the equivalent job script for Torque clusters.
  - Note that in this example, the number of threads is hard-coded in the
    source file T_hello.c! Requesting more cores than the program requires,
    doesn't make sense.
* omp1.c: For-loop parallelized with OpenMP
  - Compiling with the Intel compilers:
    + Load an intel module
    + icc -O2 -xHost -qopenmp omp1.c -o omp1
      The -qopenmp flag enables OpenMP parallelisation by the compiler.
  - Compiling with the GNU compilers
    + Load a GCC, foss or intel module (all three contain the gcc compiler)
    + gcc -O3 -march=native --openmp omp1.c -o omp1
  - Run with the included job script (Intel compiler or GCC
    from the Intel module, replace modules for other compilers).
    + omp1.slurm on Slurm systems
    + omp1.pbs on Torque systems
  - In principle, OpenMP programs get the number of threads they should use
    through the environment variable OMP_NUM_THREADS. IF not specified, the
    default in most programs is the number of cores on the node and the program
    usually fails to see that not all cores are allocated to it. However, Intel
    OpenMP usually manages to get the correct number of threads from Slurm so
    when using Slurm, OMP_NUM_THREADS is not really needed in this example.
* omp2.c: More advanced use of OpenMP, demonstrating a critical section and
  thread-private variables
  - Compiling with the Intel compilers:
    + Load an intel module
    + icc -O2 -xHost -qopenmp omp2.c -o omp2
  - Compiling with the GNU compilers
    + Load a GCC, foss or intel module (all three contain the gcc compiler)
    + gcc -O3 -march=native --openmp omp2.c -o omp2
  - Run with the included job script (Intel compiler or GCC
    from the Intel module, replace modules for other compilers).
    + omp2.slurm on Slurm systems
    + omp2.pbs on Torque systems
* omp3.c: Demonstrates reductions done through OpenMP pragmas 
  - Compiling with the Intel compilers:
    + Load an intel module
    + icc -O2 -xHost -qopenmp omp3.c -o omp3
  - Compiling with the GNU compilers
    + Load a GCC, foss or intel module (all three contain the gcc compiler)
    + gcc -O3 -march=native --openmp omp3.c -o omp3
  - Run with the included job script (Intel compiler or GCC
    from the Intel module, replace modules for other compilers)
    + omp3.slurm on Slurm systems
    + omp3.pbs on Torque systems
* mpi_hello.c: Simple "Hello, world"-program using MPI with some simple 
  communication.
  - Compiling with the Intel compilers for Intel MPI:
    + Load an intel module
    + mpiicc -O2 -xHost mpi_hello.c -o mpi_hello
    + Run with the included job script mpi_hello.slurm (Slurm systems)
      or mpi_hello.pbs (Torque systems).
  - Compiling with gcc and Intel MPI: Use the intel modules and
    + Load an intel module
    + mpigcc -O3 -march=native mpi_hello.c -o mpi_hello
    + Run with the included job script (same as above).
